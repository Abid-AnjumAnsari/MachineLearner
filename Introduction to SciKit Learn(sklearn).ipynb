{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Scikit-learn (sklearn)\n",
    "\n",
    "This notebook demonstrates some of the most useful functions of the beautiful skit learn Library.\n",
    "\n",
    "What are we going to cover :\n",
    "\n",
    "0. An end to end scikit learn workflow\n",
    "1. Getting into data ready\n",
    "2. choose the right estimator/model/algo for problems\n",
    "3. Fit the model/algorithm and use it to make predictions on our data\n",
    "4. Evaluating a model\n",
    "5. Improve a model\n",
    "6. save and load a trained model\n",
    "7. Putting it all together\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0. An end to end scikit learn workflow',\n",
       " '1. Getting into data ready',\n",
       " '2. choose the right estimator/model/algo for problems',\n",
       " '3. Fit the model/algorithm and use it to make predictions on our data',\n",
       " '4. Evaluating a model',\n",
       " '5. Improve a model',\n",
       " '6. save and load a trained model',\n",
       " '7. Putting it all together']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "What_we_are_covering =[\n",
    "\"0. An end to end scikit learn workflow\",\n",
    "\"1. Getting into data ready\",\n",
    "\"2. choose the right estimator/model/algo for problems\",\n",
    "\"3. Fit the model/algorithm and use it to make predictions on our data\",\n",
    "\"4. Evaluating a model\",\n",
    "\"5. Improve a model\",\n",
    "\"6. save and load a trained model\",\n",
    "\"7. Putting it all together\"] \n",
    "\n",
    "What_we_are_covering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standards imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.An end to end scikit-learn workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Get the data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>294</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>199</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "5   57    1   0       140   192    0        1      148      0      0.4      1   \n",
       "6   56    0   1       140   294    0        0      153      0      1.3      1   \n",
       "7   44    1   1       120   263    0        1      173      0      0.0      2   \n",
       "8   52    1   2       172   199    1        1      162      0      0.5      2   \n",
       "9   57    1   2       150   168    0        1      174      0      1.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  \n",
       "5   0     1       1  \n",
       "6   0     2       1  \n",
       "7   0     3       1  \n",
       "8   0     3       1  \n",
       "9   0     2       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.Get the data ready\n",
    "import pandas as pd\n",
    "\n",
    "heart_disease=pd.read_csv(\"heart-disease.csv\")\n",
    "heart_disease.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X (feature matrix or feature (f) jo tha or data or variable)\n",
    "\n",
    "#It will be all the column which will be used to predict the output i.e class label.\n",
    "\n",
    "#Here we have target as class label and all other columns as features.\n",
    "\n",
    "x=heart_disease.drop(\"target\",axis=1)\n",
    "\n",
    "#create y( class labels or labels) \n",
    "y=heart_disease[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Choose the right model and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 'warn',\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.Choose the right model and hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#or model\n",
    "clf=RandomForestClassifier()\n",
    "\n",
    "#We will keep the default hyperparameters\n",
    "clf.get_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Splitting the data , Fit the model to the data and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Fit the model to the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\n",
    "\n",
    "#80% data in training and 20% data is used for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>217</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>267</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>353</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "221   55    1   0       140   217    0        1      111      1      5.6   \n",
       "297   59    1   0       164   176    1        0       90      0      1.0   \n",
       "123   54    0   2       108   267    0        0      167      0      0.0   \n",
       "180   55    1   0       132   353    0        1      132      1      1.2   \n",
       "130   54    0   2       160   201    0        1      163      0      0.0   \n",
       "\n",
       "     slope  ca  thal  \n",
       "221      0   0     3  \n",
       "297      1   2     1  \n",
       "123      2   0     2  \n",
       "180      1   1     3  \n",
       "130      2   1     2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0. 2. 3. 4.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1f90466d97da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#make a prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \"\"\"\n\u001b[1;32m--> 545\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'estimators_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;31m# Check data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    362\u001b[0m                                  \"call `fit` before exploiting the model.\")\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;34m\"\"\"Validate X whenever one tries to predict, apply, predict_proba\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n\u001b[0;32m    378\u001b[0m                                 X.indptr.dtype != np.intc):\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    545\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0. 2. 3. 4.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "#make a prediction\n",
    "y_label=clf.predict(np.array([0,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since numpy array does not look like this but what does look like this is X_test\n",
    "\n",
    "y_preds_on_X_test=clf.predict(X_test)\n",
    "y_preds_on_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Evaluate the model on the training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "print(classification_report(y_test,y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improve a model\n",
    "## Try different amount of n_estimators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "for i in range(10,100,10):\n",
    "    print(f\"Trying model with {i} estimators\")\n",
    "    clf=RandomForestClassifier(n_estimators=i).fit(X_train,y_train)\n",
    "    print(f\"Model accurary on test set : {clf.score(X_test,y_test) * 100}%.2f\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.save the model and load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##6.save the model and load it\n",
    "import pickle\n",
    "\n",
    "pickle.dump(clf,open(\"random_forest_model_1.pkl\",\"wb\"))#wb=write_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(\"random_forest_model_1.pkl\",\"rb\"))\n",
    "loaded_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optional for debugging warnings\n",
    "import warnings\n",
    "#warnings.fiter(\"ignore\")\n",
    "import sklearn\n",
    "import matplotlib\n",
    "#uper me dalna \n",
    "sklearn.__version__\n",
    "matplotlib.__version__\n",
    "\n",
    "#now if something does not work for that version how to update the version..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What_we_are_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data ready to be used with machine learning :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three main things we have to do :\n",
    "\n",
    "    1.Split the data into features and labels(Usually 'X' and 'y')\n",
    "    2. Filling (also called imputing) or disregarding missing           values\n",
    "    3. Convert non-numerical values into numerical values(also called feature encoding)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=heart_disease.drop(\"target\",axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=heart_disease[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,\n",
    "                                               y,\n",
    "                                               test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "212+91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean data -- > Transform Data --> Reduce Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 MAKE SURE ITS ALL NUMERICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_extended=pd.read_csv(\"data/car-sales-extended.csv\")\n",
    "car_sales_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_extended.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_extended.shape,type(car_sales_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split thre data into X/y\n",
    "\n",
    "X=car_sales_extended.drop(\"Price\",axis=1)\n",
    "y=car_sales_extended[\"Price\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Splitting into training and test\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build machine learning model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model=RandomForestRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since Blue is a string and ml model take only numerical value so we will convert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "categorical_features=[\"Make\",\"Colour\",\"Doors\"]#Select the feature which you want to convert into numerical value\n",
    "\n",
    "#Why Doors because it is also of category of type 3,4,5\n",
    "\n",
    "one_hot=OneHotEncoder()#instantiate the one hot encoder\n",
    "\n",
    "transformer=ColumnTransformer([(\"one_hot\",one_hot,categorical_features)],remainder=\"passthrough\") #here we say hey column_transofrmer \n",
    "#take one_hot as parameter and convert catagorical_features into numerical value and remainder will passed through leave as it is. \n",
    "\n",
    "transformed_X=transformer.fit_transform(X)#then we fit_transform on the DATA FRAME WE want to work\n",
    "\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(transformed_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way\n",
    "dummies=pd.get_dummies(car_sales_extended[[\"Make\",\"Colour\",\"Doors\"]])\n",
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Door is already a numerical value hence not converted,now our data is in 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets refit the model\n",
    "np.random.seed(42)\n",
    "X_train,X_test,y_train,y_test=train_test_split(transformed_X,y,test_size=0.2)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 what if there were missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions :\n",
    "\n",
    "    1.Fill them with some values(also known as imputation).\n",
    "    2.Remove the samples with missing data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import car_sales missing data \n",
    "car_sales_missing=pd.read_csv(\"Data/car-sales-extended-missing-data.csv\")\n",
    "car_sales_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check missing value if any\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try and convert our data to numbers\n",
    "#Create X and y\n",
    "\n",
    "X=car_sales_missing.drop(\"Price\",axis=1)\n",
    "y=car_sales_missing[\"Price\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we convert this data into numbers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "catergorical_features=[\"Make\",\"Colour\",\"Doors\"]\n",
    "one_hot=OneHotEncoder()\n",
    "\n",
    "transformer=ColumnTransformer([(\"one_hot\",one_hot,categorical_features)],remainder=\"passthrough\")\n",
    "\n",
    "transformed_X=transformer.fit_transform(X)\n",
    "transformed_X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option1 : Fill missing data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing[\"Doors\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill the \"Make\" column\n",
    "\n",
    "car_sales_missing[\"Make\"].fillna(\"missing\",inplace=True)\n",
    "\n",
    "#Fill the \"Colour\" column\n",
    "car_sales_missing[\"Colour\"].fillna(\"missing\",inplace=True)\n",
    "\n",
    "#Fill the \"Odometer (KM) column\"\n",
    "car_sales_missing[\"Odometer (KM)\"].fillna(car_sales_missing[\"Odometer (KM)\"].mean(),inplace=True)\n",
    "\n",
    "#Fill the \"Doors\" Column\n",
    "car_sales_missing[\"Doors\"].fillna(4,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with missing values for class label Price We have not filled it because it is what we want to predict\n",
    "#It is hard to predict which do not have a output value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(car_sales_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=car_sales_missing.drop(\"Price\",axis=1)\n",
    "y=car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "one_hot=OneHotEncoder()\n",
    "categorical_feature=[\"Make\",\"Colour\",\"Doors\"]\n",
    "transformer=ColumnTransformer([(\"one_hot\",one_hot,categorical_feature)],remainder=\"passthrough\")\n",
    "transformed_X=transformer.fit_transform(car_sales_missing)\n",
    "transformed_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IF we want to fill the data by scikit instead with panda "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2 :Fill missing values in a data set with pure scikit learn\n",
    "## NOTE : Fill the training and testing data seperately here we have done it but it is not recommeneded\n",
    "## Training data should be seperated from test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2: Handling missing values with scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing=pd.read_csv(\"Data/car-sales-extended-missing-data.csv\")\n",
    "car_sales_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=car_sales_missing.drop(\"Price\",axis=1)\n",
    "y=car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "800-36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "200-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now filling the missing values with scikit learn instead of pandas\n",
    "from sklearn.impute import SimpleImputer # simple imputer fn basically strategy lega aur value kya fill krna h wo lega (strategy = constant to value dena prega , strategy!=constant to value not required)\n",
    "from sklearn.compose import ColumnTransformer # Coulmntransformer basically comun ko imputer fn ke madad se transform kr dega previosuly onehotencoder se transform kiye the yha imputer se krenge \n",
    "\n",
    "#Fill Categorical values with 'missning' & numerical values with mean\n",
    "cat_imputer=SimpleImputer(strategy=\"constant\",fill_value=\"missing\") # constant h to value dena prega\n",
    "door_imputer=SimpleImputer(strategy=\"constant\",fill_value=4)#same\n",
    "num_imputer=SimpleImputer(strategy=\"mean\")#no value\n",
    "\n",
    "#Define coulums which needed to be filled by the imputer\n",
    "cat_feature=[\"Make\",\"Colour\"]#ye column me changes chahiye\n",
    "door_feature=[\"Doors\"]#isme alag se isliye kiye kyu 4 bharenge na ki missing\n",
    "num_feature=[\"Odometer (KM)\"]\n",
    "\n",
    "#Create an imputer (something that fills missing data)\n",
    "\n",
    "imputer = ColumnTransformer([# ColumnTransformer 3 types ka data lega 1st is the name of the change that we want to do, uska object reference, tisra kispe change krna h\n",
    "    (\"cat_imputer\",cat_imputer,cat_feature),\n",
    "    (\"door_imputer\",door_imputer,door_feature),\n",
    "    (\"num_imputer\",num_imputer,num_feature)\n",
    "])\n",
    "\n",
    "#Transform the data\n",
    "filtered_X_train=imputer.fit_transform(X_train)\n",
    "filtered_X_test=imputer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filled=pd.DataFrame(filtered_X_train,columns=X_train.keys())\n",
    "X_train_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filled.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_filled=pd.DataFrame(filtered_X_test,columns=X_test.keys())\n",
    "X_test_filled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_filled.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_removed=X_train_filled.drop(X_train_filled.tail(36).index)\n",
    "X_test_removed=X_test_filled.drop(X_test_filled.tail(14).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "one_hot=OneHotEncoder()\n",
    "categorical_feature=[\"Make\",\"Colour\",\"Doors\"]\n",
    "transformer=ColumnTransformer([(\"one_hot\",one_hot,categorical_feature)])\n",
    "transformed_X_train=transformer.fit_transform(X_train_removed)\n",
    "transformed_X_test=transformer.fit_transform(X_test_removed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestRegressor()\n",
    "\n",
    "model.fit(transformed_X_train,y_train)\n",
    "\n",
    "model.score(transformed_X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_removed=X_train_filled.drop(X_train_filled.tail(6).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filled.tail(6).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "200-186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will come to it later right now i have to learn. This part is remaining ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What_we_are_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Choose the right estimator/algorithm/model for our problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn uses estimator as another term for machine learning model or algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depends What kind of problem we are dealing with :\n",
    "\n",
    "* Classification - predicting whether a sample is one thing or another.\n",
    "* Regression      - Predicting a number\n",
    "\n",
    "Scikit has one resource for this\n",
    "Google : sklearn machine learning map.\n",
    "    \n",
    "https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "    \n",
    "Step 1.Check the map , check if improvement can be done   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Picking a machine leanring model for regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Boston housing dataset\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "boston=load_boston()\n",
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df=pd.DataFrame(data=boston['data'],columns=boston['feature_names'])\n",
    "boston_df[\"target\"]=pd.Series(boston[\"target\"])\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many samples ?\n",
    "\n",
    "len(boston_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scikit learn lets try ridge regression model\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "#Create the data\n",
    "X=boston_df.drop(\"target\",axis=1)\n",
    "y=boston_df[\"target\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "#Instantiate the model\n",
    "model=Ridge()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "model.score(X_test,y_test)#highest = 1, lowest = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we imporve the score ?\n",
    "\n",
    "### What if ridge is not working ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the map we came to know that if Ridge regression is not working...\n",
    "##### then we can use ensemble method --> RandomForestRegressor(aata h RandomForestClassifier aata h) map dekh lena ek bar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Set Random seed\n",
    "np.random.seed(32)\n",
    "\n",
    "#Prepare data\n",
    "X=boston_df.drop(\"target\",axis=1)\n",
    "y=boston_df[\"target\"]\n",
    "\n",
    "#Split the data\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "#Instantiate RandomForestRegressor\n",
    "rf=RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "#Evaluate the Random Forest Regressor\n",
    "\n",
    "rf.fit(X_train,y_train)\n",
    "rf.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the ridge model again\n",
    "\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Choosing an estimator for classification problem\n",
    "\n",
    "Lets go to the map : https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease=pd.read_csv(\"Data/heart-disease.csv\")\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(heart_disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Consulting the map and its says to try LinearSVC:<B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the LinearSVC estimator class\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split \n",
    "#Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#prepare the data\n",
    "X=heart_disease.drop(\"target\",axis=1)\n",
    "y=heart_disease[\"target\"]\n",
    "\n",
    "#Split the data\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "#Instantiate the model LinearSVC\n",
    "clf=LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the RandomForestClassifier estimator class\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split \n",
    "#Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "#prepare the data\n",
    "X=heart_disease.drop(\"target\",axis=1)\n",
    "y=heart_disease[\"target\"]\n",
    "\n",
    "#Split the data\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "#Instantiate the model RandomForestClassifer\n",
    "clf1=RandomForestClassifier(n_estimators=100)\n",
    "clf1.fit(X_train,y_train)\n",
    "clf1.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidbid :\n",
    "## Now we have question why we have left the other models and does not go there because : \n",
    "### 1. if we have structured data like tables the using ensemble method will give us better outcome.\n",
    "### 2. if we have unstructured data (images,audio,text) then use deeplearning or transfer learning model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What_we_are_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Fit the model/algorithm and use it to make predictions on our data :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Fitting the model to the data\n",
    "\n",
    "#### X=features,featues variable,data\n",
    "#### y=labels,class labels, target,target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the RandomForestClassifier estimator class\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split \n",
    "#Setup random seed\n",
    "np.random.seed(42)\n",
    "heart_disease=pd.read_csv(\"Data/heart-disease.csv\")\n",
    "#prepare the data\n",
    "X=heart_disease.drop(\"target\",axis=1)\n",
    "y=heart_disease[\"target\"]\n",
    "\n",
    "#Split the data\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "#Instantiate the model RandomForestClassifer\n",
    "clf1=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "\n",
    "#Fit the model to the data (Training the machine leanring model to find pattern)\n",
    "\n",
    "clf1.fit(X_train,y_train)\n",
    "\n",
    "#Evaluate the random forest classifier (Use the patterns the model has learned )\n",
    "#Xtest ko lega pattern jo find kiya h fit se us patten pe predict krta h , \n",
    "#then compare krta h with y class label aur accuracy batata h 0 aur 1 ke bich me \n",
    "\n",
    "clf1.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Make prediction using the machine learning models (Unseen data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways to make predictions :\n",
    "    \n",
    "* 1.predict()\n",
    "* 2.predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a trained model to make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(np.array[1,7,8,3,4])#this does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare predictions to the truth labels to evaluate the model\n",
    "y_preds=clf1.predict(X_test)\n",
    "np.mean(y_preds==y_test)#y_test is also refereed as a ground truth value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_preds)#three different ways to do the same things. predicting mean() values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions with predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_proba() return probabibility of classification label\n",
    "clf1.predict_proba(X_test[:5])#left bda to 0 right bda to 1 compare both predict and predict_proba()\n",
    "#Threshold is 0.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.89+0.11,0.49+0.51,0.43+0.57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.predict(X_test[:5])# 1st 0 because 0.89 on left "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict will give us a single label where as predict proba gives probablity hospital me use kr sakte threshold set krne ke liye ki 0.89 ke uper ho tabhi work krenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to use predict model for Regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `predict()` can also be used for regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston=load_boston()\n",
    "boston.keys()\n",
    "boston_df=pd.DataFrame(boston[\"data\"],columns=boston[\"feature_names\"])\n",
    "boston_df[\"target\"]=boston[\"target\"]\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(33)\n",
    "\n",
    "type(boston_df)\n",
    "#Import the data\n",
    "X= boston_df.drop(\"target\",axis=1)\n",
    "y= boston_df[\"target\"]\n",
    "\n",
    "#Split the training data and test sets\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "#Instantiate and fit model\n",
    "model=RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)\n",
    "\n",
    "#Make prediction :\n",
    "y_preds=model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the prediction to the truth\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test,y_preds)#it will give on an average how far is value from actual true value on an average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What_we_are_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating a Machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have seen prediction of our model in test data but how will we be sure if the model can be used in production or not.\n",
    "\n",
    "Three ways to evaluate Scikit-Learn models/estimators\n",
    "1. Estimator `score` method\n",
    "2. The `scoring` parameter\n",
    "3. Problem-specific-metric fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "X=heart_disease.drop(\"target\",axis=1)\n",
    "y=heart_disease[\"target\"]\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Evaluating using a `score` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston=load_boston()\n",
    "boston.keys()\n",
    "boston_df=pd.DataFrame(boston[\"data\"],columns=boston[\"feature_names\"])\n",
    "boston_df[\"target\"]=boston[\"target\"]\n",
    "boston_df.head()\n",
    "\n",
    "#Import the data\n",
    "\n",
    "X= boston_df.drop(\"target\",axis=1)\n",
    "y= boston_df[\"target\"]\n",
    "\n",
    "#Split the training data and test sets\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "#Instantiate and fit model\n",
    "\n",
    "model=RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluating a model using the `scoring` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X=heart_disease.drop(\"target\",axis=1)\n",
    "y=heart_disease[\"target\"]\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf,X,y,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf,X,y,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "#Single training and test split score\n",
    "clf_single_score=clf.score(X_test,y_test)\n",
    "\n",
    "#Take the mean of 5-fold cross-validation score\n",
    "clf_cross_val_score=np.mean(cross_val_score(clf,X,y,cv=5))#We use the average\n",
    "\n",
    "#Compare the two \n",
    "clf_single_score,clf_cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will prefer the cross validation matrix.we sitll have not used SCORING PARAMETER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default scoring parameter of classifier = mean accuracy\n",
    "\n",
    "clf.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoring parameter is by default set to None by default\n",
    "\n",
    "#When scoring is set to none it willset the dafault evaluation matrics of the clsssifier or regressor\n",
    "cross_val_score(clf,X,y,cv=5,scoring=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Classification model evaluation metrics\n",
    "\n",
    "* 1.Accuracy\n",
    "* 2.Area under ROC curve\n",
    "* 3.Confusion matrix\n",
    "* 4.Classification report\n",
    "\n",
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X=heart_disease.drop(\"target\",axis=1)\n",
    "y=heart_disease[\"target\"]\n",
    "\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "cross_val_score=cross_val_score(clf,X,y,cv=5)\n",
    "#it gives mean accuracy by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Heat Disease Classifier Cross-Validated Accuracy : {np.mean(cross_val_score)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Area under the receiver operating characteristic curve(AUC/ROC)**\n",
    "* <b>Area under curve(AUC)\n",
    "* ROC curve</b>\n",
    "\n",
    "ROC curve are a comparison of a model true positive rate(tpr) verses a model false positive rate(fpr)\n",
    "\n",
    "* <b>True Positive = model predicts 1 when truth is 1\n",
    "*    False positive = model predicts 1 when truth is  0 </b>\n",
    "* <b>True Negative= model predicts 0 when truth is 0\n",
    "*    False Negative= model predicts 0 when truth is 1</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X_test... etc\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "#Make prediction with probablities\n",
    "\n",
    "y_probs=clf.predict_proba(X_test)\n",
    "\n",
    "y_probs[:10] ,len(y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_positive=y_probs[: ,1]#Taking only the positive probablity\n",
    "y_probs_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate fpr,tpr and threshold\n",
    "\n",
    "fpr,tpr,thresholds= roc_curve(y_test,y_probs_positive)\n",
    "#Check the false positive rates\n",
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function for plotting ROC curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(fpr,tpr):\n",
    "    \"\"\"\n",
    "    Plots a ROC curve given the false positive rate(fpr)\n",
    "    and true positive rate (tpr) of a model\n",
    "    \"\"\"\n",
    "    #Plot roc curve\n",
    "    plt.plot(fpr,tpr,color=\"orange\",label=\"ROC\")\n",
    "    #Plot line with no predictive power(baseline)\n",
    "    plt.plot([0,1],[0,1],color=\"blue\",linestyle=\"--\",label=\"Guessing\")\n",
    "    \n",
    "    #Customize the plot\n",
    "    plt.xlabel(\"False Positive Rate (fpr)\")\n",
    "    plt.ylabel(\"True Positive Rate (tpr)\")\n",
    "    plt.title(\"Receiver Operating Characteristics (ROC) Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plot_roc_curve(fpr,tpr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test,y_probs_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot perfect ROC curve and AUC score\n",
    "fpr,tpr,thresholds=roc_curve(y_test,y_test)\n",
    "plot_roc_curve(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rather than using the curve we can use auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Confusion Matrix**\n",
    "\n",
    "A confusion matrix is a quick way to compare the labels a model predicts and the actual labels it was supposed to predict.\n",
    "\n",
    "In essence , giving you an idea of where a model is getting confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_preds=clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test,y_preds)# Sara metrics basically true value se prediction ko compare kr rha h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize confusion matrix with pd.crosstabs()\n",
    "\n",
    "pd.crosstab(y_test,\n",
    "            y_preds,\n",
    "            rownames=[\"Actual Labels\"],\n",
    "            colnames=[\" Predicted Labels\"]\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROW are actal labels and col are predicted labels### 3 false negative and 5 false positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "22+3+5+31 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!conda install --yes --prefix {sys.prefix} seaborn\n",
    "#to install any library on the go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a confusion matrix with Seaborn heatmap\n",
    "import seaborn as sns\n",
    "#Set the font scale\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "#Create confusion matrix\n",
    "conf_mat=confusion_matrix(y_test,y_preds)\n",
    "\n",
    "#Plot it using seaborn\n",
    "sns.heatmap(conf_mat)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_mat(conf_mat):\n",
    "    \"\"\"Plot a confusion matrix using seaborne's heatmap()\"\"\"\n",
    "    fig,ax=plt.subplots(figsize=(3,3))\n",
    "    ax=sns.heatmap(conf_mat,\n",
    "                   annot=True,#Annote the boxes with conf_mat info\n",
    "                   cbar=False)\n",
    "    \n",
    "    #Fix the broken annotations(this happend in matplotlib 3.1.1)\n",
    "    #bottom,top=ax.get_ylim()\n",
    "    #ax.set_ylim(bottom+0.5,top=0.5);\n",
    "    \n",
    "    plt.xlabel(\"True Label\")\n",
    "    plt.ylabel(\"Predicted Label\")\n",
    "    \n",
    "plot_conf_mat(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In scikit learn we can also plot confusion matrix\n",
    "\n",
    "from sklearn.metrics import plot_confuison_matrix\n",
    "\n",
    "plot_confuison_matrix(clf,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a report using classification report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test,y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sklearn-classification-report.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision : indicates the proportion of positive identifications(model predcited class 1) which were actually correct. A model which produces no false positives has a precision of 1.0.\n",
    "\n",
    "## Recall    :    indicates the proportion of actual positives which were correctly classified. A model produces no false negative has a precision of 1.0.\n",
    "\n",
    "## f1-score  : A combination of precision and recall . perfect model acheive 1.0\n",
    "## support : no of samples in which each metrics is calculated on.\n",
    "## Accuracy  : The accuracy of the model in decimal form.Perfect accuracy is 1.0\n",
    "## macro avg : imbalanced set reahega tab dekhenge avg precision,recall and F1 score between classes.\n",
    "## weighed avg : the weighted average precision,recall,F1 score between clasess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why should we learn all this evaluation instead of messing our brain ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where precision and recall becomes valuable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_true=np.zeros(10000)# 10000 people with disease testing\n",
    "disease_true[0]=1#only one positive case\n",
    "\n",
    "disease_preds=np.zeros(10000)# let model predicts every case as 0 \n",
    "\n",
    "pd.DataFrame(classification_report(disease_true,disease_preds,output_dict=True))\n",
    "\n",
    "#Massive class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#These are few metrics there are lot of metrics as well search model evaluation\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Summarize classification metrics :\n",
    "\n",
    "   * <b>Accuracy</b> is a good measure to start with if all classes are balanced(e.g same amount of samples which are labelled with 0 or 1)\n",
    "   * <b>Precision</b> and <b>recall</b> become more important when classes are imbalanced.\n",
    "   * if false positive predictions are worse than false negative prediction, aim for higher precision.\n",
    "   * if false negative preictions are worse than false positives ,aim for higher recall.\n",
    "   * F1-score is a combination of precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Regression model evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric evaluation metrics : https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "1.R^2 (Pronounced r-squared) or coefficient of determination.\n",
    "2.Mean absoulte error(MAE)\n",
    "3.Mean squared error(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2.1.1.R^2 (Pronounced r-squared) or coefficient of determination.\n",
    "\n",
    "Compares your models prediction to the mean of the targets.values can range from negative infinity (a very poor model) to 1.\n",
    "\n",
    "For Example, if all your model does is predict the mean of the targets.it's R^2 value would be 0. And if your model perfectly predicts a range of numbers it's R^2 value would be 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n",
    "\n",
    "boston=load_boston()\n",
    "\n",
    "boston_df=pd.DataFrame(boston[\"data\"],columns=boston[\"feature_names\"])\n",
    "boston_df[\"target\"]=boston[\"target\"]\n",
    "boston_df.head()\n",
    "\n",
    "X=boston_df.drop(\"target\",axis=1)\n",
    "y=boston_df[\"target\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "model=RandomForestRegressor(n_estimators=100)\n",
    "model.fit(X_train,y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#Fill an array with y_test_mean\n",
    "y_test_mean=np.full(len(y_test),y_test.mean())#y_test_mean array me hamlog mean bhar diye y_test ka lenghth bhi y_test ka hi array me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,y_test_mean)\n",
    "#1st case if y_test value will be equal to mean value at y_preds here prediction value is y_test_mean it should be zero here but it giving different results \n",
    "#we will check it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it tell us how well our model is doing nut it does not tell us how far our model is off. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2.1.2 MeanAbosluteError\n",
    "\n",
    "MAE is the average of the ablsolute differences between predictions and actual values. It gives you an idea of how wrong your models are.\n",
    "\n",
    "Asolute means sum of all the differences[column] + and - all and we get no line 2.1222..etc this tells us that on an average how much a actual value is far from + or - 2.1222 in this case.\n",
    "\n",
    "sum the absolute value and divide by total no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Absolute Error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_preds=model.predict(X_test)\n",
    "mae=mean_absolute_error(y_test,y_preds)\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "df=pd.DataFrame(data={\"actual values\":y_test,\"predicted_values\":y_preds})\n",
    "df[\"differences\"]=df[\"predicted_values\"]-df[\"actual values\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(df[\"differences\"]).sum()/102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2.1.3 Mean Squared error (MSE)\n",
    "\n",
    "MSC>>MAE because it squared the eror as well.\n",
    "\n",
    "It square the the difference column every element then add all and dvide by total no of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-093c59ac2669>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my_preds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#Mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_preds=model.predict(X_test)\n",
    "mean_squared_error(y_test,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ae33ab9b8a61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msquared\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"differences\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msquared\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "squared=np.square(df[\"differences\"]).sum()\n",
    "squared/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which of these regression evaluation metrics should we use \n",
    "r^2 is similar to accuracy.it gives us a quick indication of how well oru model might be doing.\n",
    "closer to 1.0 the better .But it does not tells exactly how wrong or how far our model is from actual value.\n",
    "\n",
    "MAE gives better indication of how far off each of our model's prediction are on average.\n",
    "\n",
    "AS for `MAE` OR `MSE` because of the wayMSE is calculated,squaring the differences between predicted values and actual values ,it amplifies larger differences.Lets say we are predicting the value of houses(which we are).\n",
    "\n",
    "\n",
    "* Pay more attenrion to MAE : When being $10,000$   off is twice as bad as being $5000$ off.\n",
    "* Pay more attention to MSE : When being $10,000$ off is more than thrice as bad a being $5000$ off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tidbid : minimize mae minimze mse maximize r^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Model Evaluation (summary)\n",
    "\n",
    "Evaluating the results of a machine learning model is as important as building one.\n",
    "\n",
    "But just like how different problems have different machine learning models, different machine learning models have different evaluation metrics.\n",
    "\n",
    "Below are some of the most important evaluation metrics you'll want to look into for classification and regression models.\n",
    "\n",
    "Classification Model Evaluation Metrics/Techniques\n",
    "\n",
    "Accuracy - The accuracy of the model in decimal form. Perfect accuracy is equal to 1.0.\n",
    "\n",
    "Precision - Indicates the proportion of positive identifications (model predicted class 1) which were actually correct. A model which produces no false positives has a precision of 1.0.\n",
    "\n",
    "Recall - Indicates the proportion of actual positives which were correctly classified. A model which produces no false negatives has a recall of 1.0.\n",
    "\n",
    "F1 score - A combination of precision and recall. A perfect model achieves an F1 score of 1.0.\n",
    "\n",
    "Confusion matrix - Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagonal line).\n",
    "\n",
    "Cross-validation - Splits your dataset into multiple parts and train and tests your model on each part then evaluates performance as an average.\n",
    "\n",
    "Classification report - Sklearn has a built-in function called classification_report() which returns some of the main classification metrics such as precision, recall and f1-score.\n",
    "\n",
    "ROC Curve - Also known as receiver operating characteristic is a plot of true positive rate versus false-positive rate.\n",
    "\n",
    "Area Under Curve (AUC) Score - The area underneath the ROC curve. A perfect model achieves an AUC score of 1.0.\n",
    "\n",
    "Which classification metric should you use?\n",
    "\n",
    "Accuracy is a good measure to start with if all classes are balanced (e.g. same amount of samples which are labelled with 0 or 1).\n",
    "\n",
    "Precision and recall become more important when classes are imbalanced.\n",
    "\n",
    "If false-positive predictions are worse than false-negatives, aim for higher precision.\n",
    "\n",
    "If false-negative predictions are worse than false-positives, aim for higher recall.\n",
    "\n",
    "F1-score is a combination of precision and recall.\n",
    "\n",
    "A confusion matrix is always a good way to visualize how a classification model is going.\n",
    "\n",
    "Regression Model Evaluation Metrics/Techniques\n",
    "\n",
    "R^2 (pronounced r-squared) or the coefficient of determination - Compares your model's predictions to the mean of the targets. Values can range from negative infinity (a very poor model) to 1. For example, if all your model does is predict the mean of the targets, its R^2 value would be 0. And if your model perfectly predicts a range of numbers it's R^2 value would be 1.\n",
    "\n",
    "Mean absolute error (MAE) - The average of the absolute differences between predictions and actual values. It gives you an idea of how wrong your predictions were.\n",
    "\n",
    "Mean squared error (MSE) - The average squared differences between predictions and actual values. Squaring the errors removes negative errors. It also amplifies outliers (samples which have larger errors).\n",
    "\n",
    "Which regression metric should you use?\n",
    "\n",
    "R2 is similar to accuracy. It gives you a quick indication of how well your model might be doing. Generally, the closer your R2 value is to 1.0, the better the model. But it doesn't really tell exactly how wrong your model is in terms of how far off each prediction is.\n",
    "\n",
    "MAE gives a better indication of how far off each of your model's predictions are on average.\n",
    "\n",
    "As for MAE or MSE, because of the way MSE is calculated, squaring the differences between predicted values and actual values, it amplifies larger differences. Let's say we're predicting the value of houses (which we are).\n",
    "\n",
    "Pay more attention to MAE: When being $10,000 off is twice as bad as being $5,000 off.\n",
    "\n",
    "Pay more attention to MSE: When being $10,000 off is more than twice as bad as being $5,000 off.\n",
    "\n",
    "For more resources on evaluating a machine learning model, be sure to check out the following resources:\n",
    "\n",
    "Scikit-Learn documentation for metrics and scoring (quantifying the quality of predictions)\n",
    "\n",
    "Beyond Accuracy: Precision and Recall by Will Koehrsen\n",
    "\n",
    "Stack Overflow answer describing MSE (mean squared error) and RSME (root mean squared error)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOW $SCORING PARAMETER$ FINALLY ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2.3 Finally using the `scoring` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "heart_disease=pd.read_csv(\"Data/heart-disease.csv\")\n",
    "X=heart_disease.drop(\"target\",axis=1)\n",
    "y=heart_disease[\"target\"]\n",
    "\n",
    "clf=RandomForestClassifier(n_estimators=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81967213, 0.90163934, 0.83606557, 0.78333333, 0.78333333])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "cv_acc=cross_val_score(clf,X,y,cv=5,scoring=None)\n",
    "\n",
    "cv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validated accuracy is 82.48%\n"
     ]
    }
   ],
   "source": [
    "#Cross-validate accuracy\n",
    "print(f\"The cross-validated accuracy is {np.mean(cv_acc)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validated accuracy is 82.48%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "cv_acc=cross_val_score(clf,X,y,cv=5,scoring=\"accuracy\")\n",
    "print(f\"The cross-validated accuracy is {np.mean(cv_acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8329547346025924"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "cv_precision=cross_val_score(clf,X,y,cv=5,scoring=\"precision\")\n",
    "np.mean(cv_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8545454545454545"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "cv_recall=cross_val_score(clf,X,y,cv=5,scoring=\"recall\")\n",
    "np.mean(cv_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8426854603423346"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "cv_f1=cross_val_score(clf,X,y,cv=5,scoring=\"f1\")\n",
    "np.mean(cv_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about our regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boston_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-1531f29a2213>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#boston_df=pd.DataFrame(boston[\"data\"],boston[\"feature_names\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mboston_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mboston\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'boston_df' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston=load_boston()\n",
    "boston.keys()\n",
    "\n",
    "#boston_df=pd.DataFrame(boston[\"data\"],boston[\"feature_names\"])\n",
    "boston_df[\"target\"]=boston[\"target\"]\n",
    "\n",
    "model=RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "cv_r2=cross_val_score(model,X,y,cv=5,scoring=None)\n",
    "cv_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "cv_r2=cross_val_score(model,X,y,cv=5,scoring=\"r2\")\n",
    "cv_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-d3ed63c828aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcv_MAE\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"neg_mean_absolute_error\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#higher the value  better neg because all value is negative\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mcv_MAE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "cv_MAE=cross_val_score(model,X,y,cv=5,scoring=\"neg_mean_absolute_error\")#higher the value  better neg because all value is negative\n",
    "cv_MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-3be8b43cfa64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcv_MSE\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"neg_mean_squared_error\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_MSE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "cv_MSE=cross_val_score(model,X,y,cv=5,scoring=\"neg_mean_squared_error\")\n",
    "np.mean(cv_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. METRIC FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Using different evaluation metrics as Sciki-learn functions\n",
    "\n",
    "##### Another way to do all the things that we did before\n",
    "\n",
    "**Classification evaluation functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier metrics on the test set\n",
      "Accuracy : 85.25%\n",
      "Precision : 89.66\n",
      "Recall : 81.25\n",
      "F1 Score : 85.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n",
    "\n",
    "X=heart_disease.drop(\"target\",axis=1)\n",
    "y=heart_disease[\"target\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "clf=RandomForestClassifier()\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Make some predictions\n",
    "\n",
    "y_preds=clf.predict(X_test)\n",
    "\n",
    "#Evaluate the classifier\n",
    "\n",
    "print(\"Classifier metrics on the test set\")\n",
    "print(f\"Accuracy : {accuracy_score(y_test,y_preds)*100:.2f}%\")\n",
    "print(f\"Precision : {precision_score(y_test,y_preds)*100:.2f}\")\n",
    "print(f\"Recall : {recall_score(y_test,y_preds)*100:.2f}\")\n",
    "print(f\"F1 Score : {f1_score(y_test,y_preds)*100:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Regression evaluation functions **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boston_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-5811571f3470>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mboston_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mboston_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'boston_df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X=boston_df.drop(\"target\",axis=1)\n",
    "y=boston_df[\"target\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "model=RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_preds=model.predict(X_test)\n",
    "\n",
    "print(\"Regression model on the test set\")\n",
    "print(f\"R^2 :{r2_score(y_test,y_preds)}\")\n",
    "print(f\"MAE :{mean_absolute_error(y_test,y_preds)}\")\n",
    "print(f\"MSE :{mean_squared_error(y_test,y_preds)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0. An end to end scikit learn workflow',\n",
       " '1. Getting into data ready',\n",
       " '2. choose the right estimator/model/algo for problems',\n",
       " '3. Fit the model/algorithm and use it to make predictions on our data',\n",
       " '4. Evaluating a model',\n",
       " '5. Improve a model',\n",
       " '6. save and load a trained model',\n",
       " '7. Putting it all together']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "What_we_are_covering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0. An end to end scikit learn workflow',\n",
       " '1. Getting into data ready',\n",
       " '2. choose the right estimator/model/algo for problems',\n",
       " '3. Fit the model/algorithm and use it to make predictions on our data',\n",
       " '4. Evaluating a model',\n",
       " '5. Improve a model',\n",
       " '6. save and load a trained model',\n",
       " '7. Putting it all together']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "What_we_are_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Improve a models\n",
    "\n",
    "First predictions = baseline predictions.\n",
    "First model = baseline model.\n",
    "\n",
    "how we can improve ?\n",
    "\n",
    "From a data perspective :\n",
    "\n",
    "* Could we collect more data ? (generally more data , the better)\n",
    "* Could we improve our data ? \n",
    "\n",
    "From a model perspective :\n",
    "\n",
    "* Is there a better model you use.?\n",
    "* Could we improve the current model ?--> Hyper-parameters\n",
    "\n",
    "Hyperparameters vs paramters:\n",
    "\n",
    "* Paramters = model find these patterns in data\n",
    "\n",
    "* Hyperparamters = settings on a model you can adjust to (potentially) improve its ability to find patterns\n",
    "\n",
    "Three ways to adjust hyperparameters :\n",
    "\n",
    "* 1.By hand\n",
    "* 2.Randomly with RandomSearchCV\n",
    "* 3.Exhaustively with GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf.get_params()\n",
    "\n",
    "# dOCUMENTS ME parameter hi hyperparamter h ,python me params hi hyper paramter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Tuning hyperparameters by hand\n",
    "\n",
    "DataSet = 100\n",
    "\n",
    "Training_set = 70-80 % (Study material)\n",
    "\n",
    "Validation_set = 10-15 % (Practice set)\n",
    "\n",
    "Test_set = 10-15% (Exam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()\n",
    "\n",
    "#Go to documentation and read paramters to what each one do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are going to try and adjust \n",
    "\n",
    "* `max_depth`\n",
    "* `max_feature`\n",
    "* `min_samples_leaf`\n",
    "* `min_samples_split`\n",
    "* `n_estimators`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(y_true,y_preds):\n",
    "    \"\"\"\n",
    "    \n",
    "    Perform evaluation comparison on y_true labels vs y_preds labels.\n",
    "    \n",
    "    \"\"\"\n",
    "    accuracy=accuracy_score(y_true,y_preds)\n",
    "    precision=precision_score(y_true,y_preds)\n",
    "    recall=recall_score(y_true,y_preds)\n",
    "    f1=f1_score(y_true,y_preds)\n",
    "    metric_dict={\"accuracy\":round(accuracy,2),\n",
    "                 \"precision\":round(precision,2),\n",
    "                 \"recall\":round(recall,2),\n",
    "                 \"f1\":round(f1,2)\n",
    "                }\n",
    "    print(f\"Acc:{accuracy*100:.2f}%\")\n",
    "    print(f\"Precision:{precision:.2f}\")\n",
    "    print(f\"Recall:{recall:.2f}\")\n",
    "    print(f\"F1:{f1:.2f}\")\n",
    "    \n",
    "    return metric_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:80.00%\n",
      "Precision:0.77\n",
      "Recall:0.92\n",
      "F1:0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8, 'precision': 0.77, 'recall': 0.92, 'f1': 0.84}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have to manually split the data into train and test set.\n",
    "heart_disease\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "#shuffle the data why because we want to make sure we take random data for our splits\n",
    "\n",
    "heart_disease_shuffled=heart_disease.sample(frac=1)\n",
    "\n",
    "#shuffle into X & y\n",
    "\n",
    "X=heart_disease_shuffled.drop(\"target\",axis=1)\n",
    "y=heart_disease_shuffled[\"target\"]\n",
    "\n",
    "#Split the data into train,validation & test sets\n",
    "train_split = round(0.7 * len(heart_disease_shuffled)) #70% of the data \n",
    "valid_split=  round(train_split +0.15*len(heart_disease_shuffled)) #70+15 = 85 % of the data\n",
    "\n",
    "X_train,y_train = X[:train_split],y[:train_split]\n",
    "X_valid,y_valid = X[train_split:valid_split],y[train_split:valid_split]\n",
    "X_test,y_test= X[valid_split:],y[valid_split:]\n",
    "\n",
    "len(X_train),len(X_valid),len(X_test)\n",
    "\n",
    "clf=RandomForestClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Make prediction\n",
    "y_preds=clf.predict(X_valid)\n",
    "\n",
    "#Evaluate the classifier on validation set\n",
    "baseline_metrics=evaluate_preds(y_valid,y_preds)\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:82.22%\n",
      "Precision:0.84\n",
      "Recall:0.84\n",
      "F1:0.84\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.82, 'precision': 0.84, 'recall': 0.84, 'f1': 0.84}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Create a second classifier with different hyperparameters\n",
    "\n",
    "clf2=RandomForestClassifier(n_estimators=100)\n",
    "clf2.fit(X_train,y_train)\n",
    "\n",
    "#Make predictions\n",
    "y_preds_2=clf2.predict(X_valid)\n",
    "\n",
    "#Make the 2nd classifier metrics\n",
    "clf2_metrics=evaluate_preds(y_valid,y_preds_2)\n",
    "clf2_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3=RandomForestClassifier(n_estimators=100,max_depth=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Hyperparameter tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] n_estimators=1200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=1200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=5, total=   1.6s\n",
      "[CV] n_estimators=1200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=1200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=5, total=   1.7s\n",
      "[CV] n_estimators=1200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=1200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=5, total=   1.5s\n",
      "[CV] n_estimators=1200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=1200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=5, total=   1.5s\n",
      "[CV] n_estimators=1200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=1200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=5, total=   1.5s\n",
      "[CV] n_estimators=100, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=30 \n",
      "[CV]  n_estimators=100, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=30, total=   0.0s\n",
      "[CV] n_estimators=100, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=30 \n",
      "[CV]  n_estimators=100, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=30, total=   0.1s\n",
      "[CV] n_estimators=100, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=30 \n",
      "[CV]  n_estimators=100, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=30, total=   0.0s\n",
      "[CV] n_estimators=100, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=30 \n",
      "[CV]  n_estimators=100, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=30, total=   0.0s\n",
      "[CV] n_estimators=100, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=30 \n",
      "[CV]  n_estimators=100, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=30, total=   0.0s\n",
      "[CV] n_estimators=200, min_samples_split=2, min_samples_leaf=2, max_features=sqrt, max_depth=10 \n",
      "[CV]  n_estimators=200, min_samples_split=2, min_samples_leaf=2, max_features=sqrt, max_depth=10, total=   0.2s\n",
      "[CV] n_estimators=200, min_samples_split=2, min_samples_leaf=2, max_features=sqrt, max_depth=10 \n",
      "[CV]  n_estimators=200, min_samples_split=2, min_samples_leaf=2, max_features=sqrt, max_depth=10, total=   0.2s\n",
      "[CV] n_estimators=200, min_samples_split=2, min_samples_leaf=2, max_features=sqrt, max_depth=10 \n",
      "[CV]  n_estimators=200, min_samples_split=2, min_samples_leaf=2, max_features=sqrt, max_depth=10, total=   0.2s\n",
      "[CV] n_estimators=200, min_samples_split=2, min_samples_leaf=2, max_features=sqrt, max_depth=10 \n",
      "[CV]  n_estimators=200, min_samples_split=2, min_samples_leaf=2, max_features=sqrt, max_depth=10, total=   0.2s\n",
      "[CV] n_estimators=200, min_samples_split=2, min_samples_leaf=2, max_features=sqrt, max_depth=10 \n",
      "[CV]  n_estimators=200, min_samples_split=2, min_samples_leaf=2, max_features=sqrt, max_depth=10, total=   0.2s\n",
      "[CV] n_estimators=100, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=100, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=20, total=   0.0s\n",
      "[CV] n_estimators=100, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=100, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=20, total=   0.1s\n",
      "[CV] n_estimators=100, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=100, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=20, total=   0.0s\n",
      "[CV] n_estimators=100, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=100, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=20, total=   0.1s\n",
      "[CV] n_estimators=100, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=100, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=20, total=   0.0s\n",
      "[CV] n_estimators=10, min_samples_split=4, min_samples_leaf=1, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=10, min_samples_split=4, min_samples_leaf=1, max_features=sqrt, max_depth=5, total=   0.0s\n",
      "[CV] n_estimators=10, min_samples_split=4, min_samples_leaf=1, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=10, min_samples_split=4, min_samples_leaf=1, max_features=sqrt, max_depth=5, total=   0.0s\n",
      "[CV] n_estimators=10, min_samples_split=4, min_samples_leaf=1, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=10, min_samples_split=4, min_samples_leaf=1, max_features=sqrt, max_depth=5, total=   0.0s\n",
      "[CV] n_estimators=10, min_samples_split=4, min_samples_leaf=1, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=10, min_samples_split=4, min_samples_leaf=1, max_features=sqrt, max_depth=5, total=   0.0s\n",
      "[CV] n_estimators=10, min_samples_split=4, min_samples_leaf=1, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=10, min_samples_split=4, min_samples_leaf=1, max_features=sqrt, max_depth=5, total=   0.0s\n",
      "[CV] n_estimators=10, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10 \n",
      "[CV]  n_estimators=10, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10, total=   0.0s\n",
      "[CV] n_estimators=10, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10 \n",
      "[CV]  n_estimators=10, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10, total=   0.0s\n",
      "[CV] n_estimators=10, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10 \n",
      "[CV]  n_estimators=10, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10, total=   0.0s\n",
      "[CV] n_estimators=10, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10 \n",
      "[CV]  n_estimators=10, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10, total=   0.0s\n",
      "[CV] n_estimators=10, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10 \n",
      "[CV]  n_estimators=10, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10, total=   0.0s\n",
      "[CV] n_estimators=500, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None \n",
      "[CV]  n_estimators=500, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None, total=   0.5s\n",
      "[CV] n_estimators=500, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None \n",
      "[CV]  n_estimators=500, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None, total=   0.6s\n",
      "[CV] n_estimators=500, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None \n",
      "[CV]  n_estimators=500, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None, total=   0.7s\n",
      "[CV] n_estimators=500, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None \n",
      "[CV]  n_estimators=500, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None, total=   0.6s\n",
      "[CV] n_estimators=500, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None \n",
      "[CV]  n_estimators=500, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None, total=   0.6s\n",
      "[CV] n_estimators=200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None \n",
      "[CV]  n_estimators=200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None, total=   0.2s\n",
      "[CV] n_estimators=200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None \n",
      "[CV]  n_estimators=200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None, total=   0.2s\n",
      "[CV] n_estimators=200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None \n",
      "[CV]  n_estimators=200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None, total=   0.2s\n",
      "[CV] n_estimators=200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None \n",
      "[CV]  n_estimators=200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None, total=   0.2s\n",
      "[CV] n_estimators=200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None \n",
      "[CV]  n_estimators=200, min_samples_split=6, min_samples_leaf=2, max_features=sqrt, max_depth=None, total=   0.2s\n",
      "[CV] n_estimators=200, min_samples_split=4, min_samples_leaf=4, max_features=auto, max_depth=10 \n",
      "[CV]  n_estimators=200, min_samples_split=4, min_samples_leaf=4, max_features=auto, max_depth=10, total=   0.2s\n",
      "[CV] n_estimators=200, min_samples_split=4, min_samples_leaf=4, max_features=auto, max_depth=10 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=200, min_samples_split=4, min_samples_leaf=4, max_features=auto, max_depth=10, total=   0.3s\n",
      "[CV] n_estimators=200, min_samples_split=4, min_samples_leaf=4, max_features=auto, max_depth=10 \n",
      "[CV]  n_estimators=200, min_samples_split=4, min_samples_leaf=4, max_features=auto, max_depth=10, total=   0.2s\n",
      "[CV] n_estimators=200, min_samples_split=4, min_samples_leaf=4, max_features=auto, max_depth=10 \n",
      "[CV]  n_estimators=200, min_samples_split=4, min_samples_leaf=4, max_features=auto, max_depth=10, total=   0.2s\n",
      "[CV] n_estimators=200, min_samples_split=4, min_samples_leaf=4, max_features=auto, max_depth=10 \n",
      "[CV]  n_estimators=200, min_samples_split=4, min_samples_leaf=4, max_features=auto, max_depth=10, total=   0.2s\n",
      "[CV] n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20 \n",
      "[CV]  n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20, total=   1.3s\n",
      "[CV] n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20 \n",
      "[CV]  n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20, total=   1.3s\n",
      "[CV] n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20 \n",
      "[CV]  n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20, total=   1.3s\n",
      "[CV] n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20 \n",
      "[CV]  n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20, total=   1.3s\n",
      "[CV] n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20 \n",
      "[CV]  n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20, total=   1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   27.0s finished\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "#Create a dictionary with hyper paramters we want to test and values for hyperparamters\n",
    "grid={\"n_estimators\":[10,100,200,500,1000,1200],\n",
    "      \"max_depth\":[None,5,10,20,30],\n",
    "      \"max_features\":[\"auto\",\"sqrt\"],\n",
    "      \"min_samples_split\":[2,4,6],\n",
    "      \"min_samples_leaf\":[1,2,4]\n",
    "     }\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split into X and y\n",
    "X=heart_disease_shuffled.drop(\"target\",axis=1)\n",
    "y=heart_disease_shuffled[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "#Instantiate RandomForestClassifier(n_jobs)\n",
    "clf=RandomForestClassifier(n_jobs=1)\n",
    "\n",
    "#Setup RandomizedSearchCV\n",
    "rs_clf=RandomizedSearchCV(estimator=clf,\n",
    "                         param_distributions=grid,\n",
    "                         n_iter=10,#numbers of models to try\n",
    "                          cv=5,\n",
    "                          verbose=2\n",
    "                         )\n",
    "#This function will run for 10 times and will take random parameters from param_distributions and also choose the best parametrs for us\n",
    "\n",
    "rs_clf.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100,\n",
       " 'min_samples_split': 6,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 5}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_clf.best_params_# it will give the best set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:81.97%\n",
      "Precision:0.76\n",
      "Recall:0.89\n",
      "F1:0.82\n"
     ]
    }
   ],
   "source": [
    "#Make prediction with the best hyperparameters\n",
    "\n",
    "rs_y_preds=rs_clf.predict(X_test)\n",
    "\n",
    "#Evaluate the prediction\n",
    "rs_metrics=evaluate_preds(y_test,rs_y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Workflow first method ->2nd method-->3rd method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 HyperParamter tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [10, 100, 200, 500, 1000, 1200],\n",
       " 'max_depth': [None, 5, 10, 20, 30],\n",
       " 'max_features': ['auto', 'sqrt'],\n",
       " 'min_samples_split': [2, 4, 6],\n",
       " 'min_samples_leaf': [1, 2, 4]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Difference is that it will go to all combination\n",
    "grid # it will give all hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6*5*2*3*3*5  and It will create 2700 model which will take huge computational power so what \n",
    "##### we will do is we will use the best parametrs from RandomSearchCV and put it in GridSearch CV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2={'n_estimators': [100, 200, 500],\n",
    " 'max_depth': [None],\n",
    " 'max_features': ['auto','sqrt'],\n",
    " 'min_samples_split': [6],\n",
    " 'min_samples_leaf': [1,2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*1*2*1*2*5 # 60 models which are less right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100, total=   0.0s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100, total=   0.1s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100, total=   0.0s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100, total=   0.0s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=100, total=   0.0s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500, total=   0.7s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500, total=   0.6s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500, total=   0.8s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500, total=   0.8s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500, total=   0.9s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=100, total=   0.1s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=100, total=   0.1s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=100, total=   0.1s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=100, total=   0.1s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=100, total=   0.0s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=500, total=   0.6s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=500, total=   0.6s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=500, total=   0.8s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=500, total=   0.9s\n",
      "[CV] max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=500, total=   0.8s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=100, total=   0.0s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=100, total=   0.1s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=100, total=   0.1s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=100, total=   0.1s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=100, total=   0.1s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=200, total=   0.3s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500, total=   0.6s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500, total=   0.6s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500, total=   0.7s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500, total=   0.7s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500, total=   0.9s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=100, total=   0.0s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=100, total=   0.1s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=100, total=   0.0s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=100, total=   0.1s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=100 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=100, total=   0.0s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=200, total=   0.2s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500, total=   0.6s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500, total=   0.6s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500, total=   0.7s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500, total=   0.8s\n",
      "[CV] max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500 \n",
      "[CV]  max_depth=None, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, n_estimators=500, total=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   28.1s finished\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split into X and y\n",
    "X=heart_disease_shuffled.drop(\"target\",axis=1)\n",
    "y=heart_disease_shuffled[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "#Instantiate RandomForestClassifier(n_jobs)\n",
    "clf=RandomForestClassifier(n_jobs=1)\n",
    "\n",
    "#Setup RandomizedSearchCV\n",
    "gs_clf=GridSearchCV(estimator=clf,\n",
    "                         param_grid=grid2,\n",
    "                         cv=5,verbose=2\n",
    "                         )\n",
    "#This function will run for  times and will take random parameters from param_distributions and also choose the best parametrs for us\n",
    "\n",
    "gs_clf.fit(X_train,y_train);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 6,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:78.69%\n",
      "Precision:0.74\n",
      "Recall:0.82\n",
      "F1:0.78\n"
     ]
    }
   ],
   "source": [
    "gs_y_preds=gs_clf.predict(X_test)\n",
    "\n",
    "#evaluate the predictions\n",
    "gs_metrics=evaluate_preds(y_test,gs_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Let's compare our different models metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "compare_metrics=pd.DataFrame({\"baseline\":baseline_metrics,\n",
    "                              \"clf2\" :clf2_metrics,\n",
    "                              \"random search\":rs_metrics,\n",
    "                              \"grid_search\":gs_metrics})\n",
    "\n",
    "compare_metrics.plot.bar(figsize=(10,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0. An end to end scikit learn workflow',\n",
       " '1. Getting into data ready',\n",
       " '2. choose the right estimator/model/algo for problems',\n",
       " '3. Fit the model/algorithm and use it to make predictions on our data',\n",
       " '4. Evaluating a model',\n",
       " '5. Improve a model',\n",
       " '6. save and load a trained model',\n",
       " '7. Putting it all together']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "What_we_are_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Save and Load a trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two ways to save and load machine learning models:\n",
    "\n",
    "  1.with Python's `pickle` module\n",
    " \n",
    "  2.with the `joblibs` module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1 Pickle**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**save a model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "#save an existing model to file\n",
    "\n",
    "pickle.dump(gs_clf,open(\"gs_random_forest_model_1.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load a saved model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pickle_model=pickle.load(open(\"gs_random_forest_model_1.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:78.69%\n",
      "Precision:0.74\n",
      "Recall:0.82\n",
      "F1:0.78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.79, 'precision': 0.74, 'recall': 0.82, 'f1': 0.78}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Makle some prediction\n",
    "\n",
    "pickle_y_preds=load_pickle_model.predict(X_test)\n",
    "evaluate_preds(y_test,pickle_y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc:78.69%\n",
    "Precision:0.74\n",
    "Recall:0.82\n",
    "F1:0.78\n",
    "    \n",
    "same as original model input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2 joblib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs_random_forest_model_1.joblib']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump,load\n",
    "\n",
    "#save model to file\n",
    "dump(gs_clf,filename=\"gs_random_forest_model_1.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import a saved joblib model\n",
    "load_joblib_model=load(filename=\"gs_random_forest_model_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:78.69%\n",
      "Precision:0.74\n",
      "Recall:0.82\n",
      "F1:0.78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.79, 'precision': 0.74, 'recall': 0.82, 'f1': 0.78}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make and evaluate joblib predictions\n",
    "joblib_preds=load_joblib_model.predict(X_test)\n",
    "\n",
    "evaluate_preds(y_test,joblib_preds)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to sckikit learn model if model is large use joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0. An end to end scikit learn workflow',\n",
       " '1. Getting into data ready',\n",
       " '2. choose the right estimator/model/algo for problems',\n",
       " '3. Fit the model/algorithm and use it to make predictions on our data',\n",
       " '4. Evaluating a model',\n",
       " '5. Improve a model',\n",
       " '6. save and load a trained model',\n",
       " '7. Putting it all together']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "What_we_are_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Putting in all together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Odometer (KM)</th>\n",
       "      <th>Doors</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>35431.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>Blue</td>\n",
       "      <td>192714.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19943.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>84714.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>White</td>\n",
       "      <td>154365.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13434.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Blue</td>\n",
       "      <td>181577.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14043.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Honda</td>\n",
       "      <td>Red</td>\n",
       "      <td>42652.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23883.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Blue</td>\n",
       "      <td>163453.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8473.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20306.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>White</td>\n",
       "      <td>130538.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9374.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Honda</td>\n",
       "      <td>Blue</td>\n",
       "      <td>51029.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26683.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>White</td>\n",
       "      <td>167421.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Green</td>\n",
       "      <td>17119.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>White</td>\n",
       "      <td>102303.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16909.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>White</td>\n",
       "      <td>134181.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Honda</td>\n",
       "      <td>Blue</td>\n",
       "      <td>199833.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18946.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Blue</td>\n",
       "      <td>205592.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16290.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Red</td>\n",
       "      <td>96742.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>34465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BMW</td>\n",
       "      <td>White</td>\n",
       "      <td>194189.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17177.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>White</td>\n",
       "      <td>67991.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Blue</td>\n",
       "      <td>215820.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124844.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Honda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30615.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29653.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>White</td>\n",
       "      <td>148744.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22489.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Honda</td>\n",
       "      <td>Green</td>\n",
       "      <td>130075.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Honda</td>\n",
       "      <td>Blue</td>\n",
       "      <td>172718.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Honda</td>\n",
       "      <td>Blue</td>\n",
       "      <td>125819.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15686.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>180390.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13344.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Honda</td>\n",
       "      <td>Green</td>\n",
       "      <td>82783.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10984.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>56687.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>White</td>\n",
       "      <td>112004.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13586.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Blue</td>\n",
       "      <td>186309.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16416.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>BMW</td>\n",
       "      <td>Black</td>\n",
       "      <td>178164.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24891.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17939.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>Honda</td>\n",
       "      <td>Green</td>\n",
       "      <td>237627.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>NaN</td>\n",
       "      <td>White</td>\n",
       "      <td>155383.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14345.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>Honda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22409.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10429.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Blue</td>\n",
       "      <td>95317.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7435.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Blue</td>\n",
       "      <td>128016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16835.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>BMW</td>\n",
       "      <td>White</td>\n",
       "      <td>85739.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>48419.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Black</td>\n",
       "      <td>17975.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17940.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Blue</td>\n",
       "      <td>230314.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6720.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>White</td>\n",
       "      <td>129454.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6446.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>238172.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Red</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14671.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Blue</td>\n",
       "      <td>157235.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Blue</td>\n",
       "      <td>216250.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9691.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>71934.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26882.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>215235.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Black</td>\n",
       "      <td>248736.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8358.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Red</td>\n",
       "      <td>41735.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13928.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>White</td>\n",
       "      <td>173408.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8082.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>Honda</td>\n",
       "      <td>Blue</td>\n",
       "      <td>235985.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>Honda</td>\n",
       "      <td>Green</td>\n",
       "      <td>54721.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27419.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Black</td>\n",
       "      <td>162523.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4696.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>BMW</td>\n",
       "      <td>Blue</td>\n",
       "      <td>163322.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31666.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Black</td>\n",
       "      <td>35820.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32042.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>White</td>\n",
       "      <td>155144.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5716.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Blue</td>\n",
       "      <td>66604.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>31570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>215883.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Blue</td>\n",
       "      <td>248360.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12732.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Make Colour  Odometer (KM)  Doors    Price\n",
       "0     Honda  White        35431.0    4.0  15323.0\n",
       "1       BMW   Blue       192714.0    5.0  19943.0\n",
       "2     Honda  White        84714.0    4.0  28343.0\n",
       "3    Toyota  White       154365.0    4.0  13434.0\n",
       "4    Nissan   Blue       181577.0    3.0  14043.0\n",
       "5     Honda    Red        42652.0    4.0  23883.0\n",
       "6    Toyota   Blue       163453.0    4.0   8473.0\n",
       "7     Honda  White            NaN    4.0  20306.0\n",
       "8       NaN  White       130538.0    4.0   9374.0\n",
       "9     Honda   Blue        51029.0    4.0  26683.0\n",
       "10   Nissan  White       167421.0    4.0  16259.0\n",
       "11   Nissan  Green        17119.0    4.0   6160.0\n",
       "12   Nissan  White       102303.0    4.0  16909.0\n",
       "13      NaN  White       134181.0    4.0  11121.0\n",
       "14    Honda   Blue       199833.0    4.0  18946.0\n",
       "15   Toyota   Blue       205592.0    4.0  16290.0\n",
       "16   Toyota    Red        96742.0    4.0  34465.0\n",
       "17      BMW  White       194189.0    5.0  17177.0\n",
       "18   Nissan  White        67991.0    3.0   9109.0\n",
       "19   Nissan   Blue       215820.0    4.0   6010.0\n",
       "20   Toyota    NaN       124844.0    4.0  24130.0\n",
       "21    Honda    NaN        30615.0    4.0  29653.0\n",
       "22   Toyota  White       148744.0    4.0  22489.0\n",
       "23    Honda  Green       130075.0    4.0  21242.0\n",
       "24    Honda   Blue       172718.0    4.0  14274.0\n",
       "25    Honda   Blue       125819.0    4.0  15686.0\n",
       "26    Honda  White       180390.0    4.0  13344.0\n",
       "27    Honda  Green        82783.0    4.0  10984.0\n",
       "28    Honda  White        56687.0    4.0   6135.0\n",
       "29   Toyota  White       112004.0    4.0  13586.0\n",
       "..      ...    ...            ...    ...      ...\n",
       "970  Toyota   Blue       186309.0    4.0  16416.0\n",
       "971     BMW  Black       178164.0    3.0  24891.0\n",
       "972   Honda  White            NaN    4.0  17939.0\n",
       "973   Honda  Green       237627.0    4.0   8430.0\n",
       "974     NaN  White       155383.0    4.0  14345.0\n",
       "975   Honda    NaN        22409.0    4.0  10429.0\n",
       "976  Toyota   Blue        95317.0    4.0   7435.0\n",
       "977  Toyota   Blue       128016.0    4.0  16835.0\n",
       "978     BMW  White        85739.0    5.0  48419.0\n",
       "979  Toyota  Black        17975.0    4.0  17940.0\n",
       "980  Toyota   Blue       230314.0    4.0   6720.0\n",
       "981  Toyota  White       129454.0    4.0   6446.0\n",
       "982   Honda  White       238172.0    4.0  13273.0\n",
       "983  Toyota    Red            NaN    4.0  14671.0\n",
       "984  Nissan   Blue       157235.0    4.0   4196.0\n",
       "985     NaN   Blue       216250.0    4.0   9691.0\n",
       "986   Honda  White        71934.0    4.0  26882.0\n",
       "987   Honda  White       215235.0    4.0   3825.0\n",
       "988  Nissan  Black       248736.0    4.0   8358.0\n",
       "989  Toyota    Red        41735.0    4.0  13928.0\n",
       "990  Toyota  White       173408.0    4.0   8082.0\n",
       "991   Honda   Blue       235985.0    4.0   9184.0\n",
       "992   Honda  Green        54721.0    4.0  27419.0\n",
       "993  Nissan  Black       162523.0    4.0   4696.0\n",
       "994     BMW   Blue       163322.0    3.0  31666.0\n",
       "995  Toyota  Black        35820.0    4.0  32042.0\n",
       "996     NaN  White       155144.0    3.0   5716.0\n",
       "997  Nissan   Blue        66604.0    4.0  31570.0\n",
       "998   Honda  White       215883.0    4.0   4001.0\n",
       "999  Toyota   Blue       248360.0    4.0  12732.0\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"data/car-sales-extended-missing-data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make              object\n",
       "Colour            object\n",
       "Odometer (KM)    float64\n",
       "Doors            float64\n",
       "Price            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Make             49\n",
       "Colour           50\n",
       "Odometer (KM)    50\n",
       "Doors            50\n",
       "Price            50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This time we will be doing using scikit pipeline\n",
    "\n",
    "### Steps we want to do (all in one cell):\n",
    "**1.Fill missing data**\n",
    "\n",
    "**2.Convert data to numbers**\n",
    "\n",
    "\n",
    "**3.Build a model on the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1821575815702311"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting data ready\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Modelling\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "\n",
    "#Setup random seed\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "#Import data and drop rows with missing lables\n",
    "data=pd.read_csv(\"data/car-sales-extended-missing-data.csv\")\n",
    "data.dropna(subset=[\"Price\"],inplace=True)\n",
    "\n",
    "#Define different features and transformer pipeline\n",
    "categorical_features=[\"Make\", \"Colour\"]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\",SimpleImputer(strategy=\"constant\",fill_value=\"missing\")),\n",
    "    (\"onehot\",OneHotEncoder(handle_unknown=\"ignore\"))                                   \n",
    "                                       ])\n",
    "door_features=[\"Doors\"]\n",
    "door_transformer = Pipeline(steps=[\n",
    "    (\"imputer\",SimpleImputer(strategy=\"constant\",fill_value=4))\n",
    "])\n",
    "\n",
    "numeric_features = [\"Odometer (KM)\"]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\",SimpleImputer(strategy=\"mean\"))\n",
    "                                   ])\n",
    "#Setup the preprocessing steps (fill the missing values, then convert to numbers)\n",
    "preprocessor=ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"cat\",categorical_transformer,categorical_features),\n",
    "    (\"door\",door_transformer,door_features),\n",
    "    (\"num\",numeric_transformer,numeric_features)\n",
    "    \n",
    "])\n",
    "\n",
    "#Create a preprocessing and modelling pipeline\n",
    "model=Pipeline(steps=[(\"preprocessor\",preprocessor),\n",
    "                     (\"model\",RandomForestRegressor())\n",
    "                     ])\n",
    "\n",
    "#Split the data\n",
    "X=data.drop(\"Price\",axis=1)\n",
    "y=data[\"Price\"]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "\n",
    "#Fit and score the model\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to use `GridSearchCV` or `RandomizedSearchCV` with our `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocessor',\n",
       "   ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "            transformer_weights=None,\n",
       "            transformers=[('cat', Pipeline(memory=None,\n",
       "        steps=[('imputer', SimpleImputer(copy=True, fill_value='missing', missing_values=nan,\n",
       "          strategy='constant', verbose=0)), ('onehot', OneHotEncoder(categorical_features=None, categories=None,\n",
       "          dtype=<class 'numpy.float64'>, handle_unknown='i...rue, fill_value=None, missing_values=nan, strategy='mean',\n",
       "          verbose=0))]), ['Odometer (KM)'])])),\n",
       "  ('model',\n",
       "   RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "              max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "              oob_score=False, random_state=None, verbose=0, warm_start=False))],\n",
       " 'preprocessor': ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "          transformer_weights=None,\n",
       "          transformers=[('cat', Pipeline(memory=None,\n",
       "      steps=[('imputer', SimpleImputer(copy=True, fill_value='missing', missing_values=nan,\n",
       "        strategy='constant', verbose=0)), ('onehot', OneHotEncoder(categorical_features=None, categories=None,\n",
       "        dtype=<class 'numpy.float64'>, handle_unknown='i...rue, fill_value=None, missing_values=nan, strategy='mean',\n",
       "        verbose=0))]), ['Odometer (KM)'])]),\n",
       " 'model': RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       " 'preprocessor__n_jobs': None,\n",
       " 'preprocessor__remainder': 'drop',\n",
       " 'preprocessor__sparse_threshold': 0.3,\n",
       " 'preprocessor__transformer_weights': None,\n",
       " 'preprocessor__transformers': [('cat', Pipeline(memory=None,\n",
       "        steps=[('imputer', SimpleImputer(copy=True, fill_value='missing', missing_values=nan,\n",
       "          strategy='constant', verbose=0)), ('onehot', OneHotEncoder(categorical_features=None, categories=None,\n",
       "          dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "          n_values=None, sparse=True))]), ['Make', 'Colour']),\n",
       "  ('door', Pipeline(memory=None,\n",
       "        steps=[('imputer', SimpleImputer(copy=True, fill_value=4, missing_values=nan,\n",
       "          strategy='constant', verbose=0))]), ['Doors']),\n",
       "  ('num', Pipeline(memory=None,\n",
       "        steps=[('imputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean',\n",
       "          verbose=0))]), ['Odometer (KM)'])],\n",
       " 'preprocessor__cat': Pipeline(memory=None,\n",
       "      steps=[('imputer', SimpleImputer(copy=True, fill_value='missing', missing_values=nan,\n",
       "        strategy='constant', verbose=0)), ('onehot', OneHotEncoder(categorical_features=None, categories=None,\n",
       "        dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "        n_values=None, sparse=True))]),\n",
       " 'preprocessor__door': Pipeline(memory=None,\n",
       "      steps=[('imputer', SimpleImputer(copy=True, fill_value=4, missing_values=nan,\n",
       "        strategy='constant', verbose=0))]),\n",
       " 'preprocessor__num': Pipeline(memory=None,\n",
       "      steps=[('imputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean',\n",
       "        verbose=0))]),\n",
       " 'preprocessor__cat__memory': None,\n",
       " 'preprocessor__cat__steps': [('imputer',\n",
       "   SimpleImputer(copy=True, fill_value='missing', missing_values=nan,\n",
       "          strategy='constant', verbose=0)),\n",
       "  ('onehot', OneHotEncoder(categorical_features=None, categories=None,\n",
       "          dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "          n_values=None, sparse=True))],\n",
       " 'preprocessor__cat__imputer': SimpleImputer(copy=True, fill_value='missing', missing_values=nan,\n",
       "        strategy='constant', verbose=0),\n",
       " 'preprocessor__cat__onehot': OneHotEncoder(categorical_features=None, categories=None,\n",
       "        dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "        n_values=None, sparse=True),\n",
       " 'preprocessor__cat__imputer__copy': True,\n",
       " 'preprocessor__cat__imputer__fill_value': 'missing',\n",
       " 'preprocessor__cat__imputer__missing_values': nan,\n",
       " 'preprocessor__cat__imputer__strategy': 'constant',\n",
       " 'preprocessor__cat__imputer__verbose': 0,\n",
       " 'preprocessor__cat__onehot__categorical_features': None,\n",
       " 'preprocessor__cat__onehot__categories': None,\n",
       " 'preprocessor__cat__onehot__dtype': numpy.float64,\n",
       " 'preprocessor__cat__onehot__handle_unknown': 'ignore',\n",
       " 'preprocessor__cat__onehot__n_values': None,\n",
       " 'preprocessor__cat__onehot__sparse': True,\n",
       " 'preprocessor__door__memory': None,\n",
       " 'preprocessor__door__steps': [('imputer',\n",
       "   SimpleImputer(copy=True, fill_value=4, missing_values=nan,\n",
       "          strategy='constant', verbose=0))],\n",
       " 'preprocessor__door__imputer': SimpleImputer(copy=True, fill_value=4, missing_values=nan,\n",
       "        strategy='constant', verbose=0),\n",
       " 'preprocessor__door__imputer__copy': True,\n",
       " 'preprocessor__door__imputer__fill_value': 4,\n",
       " 'preprocessor__door__imputer__missing_values': nan,\n",
       " 'preprocessor__door__imputer__strategy': 'constant',\n",
       " 'preprocessor__door__imputer__verbose': 0,\n",
       " 'preprocessor__num__memory': None,\n",
       " 'preprocessor__num__steps': [('imputer',\n",
       "   SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean',\n",
       "          verbose=0))],\n",
       " 'preprocessor__num__imputer': SimpleImputer(copy=True, fill_value=None, missing_values=nan, strategy='mean',\n",
       "        verbose=0),\n",
       " 'preprocessor__num__imputer__copy': True,\n",
       " 'preprocessor__num__imputer__fill_value': None,\n",
       " 'preprocessor__num__imputer__missing_values': nan,\n",
       " 'preprocessor__num__imputer__strategy': 'mean',\n",
       " 'preprocessor__num__imputer__verbose': 0,\n",
       " 'model__bootstrap': True,\n",
       " 'model__criterion': 'mse',\n",
       " 'model__max_depth': None,\n",
       " 'model__max_features': 'auto',\n",
       " 'model__max_leaf_nodes': None,\n",
       " 'model__min_impurity_decrease': 0.0,\n",
       " 'model__min_impurity_split': None,\n",
       " 'model__min_samples_leaf': 1,\n",
       " 'model__min_samples_split': 2,\n",
       " 'model__min_weight_fraction_leaf': 0.0,\n",
       " 'model__n_estimators': 10,\n",
       " 'model__n_jobs': None,\n",
       " 'model__oob_score': False,\n",
       " 'model__random_state': None,\n",
       " 'model__verbose': 0,\n",
       " 'model__warm_start': False}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.3s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.3s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.3s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.3s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   3.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   2.4s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   2.8s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   2.5s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   2.5s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   2.9s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   3.0s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   2.8s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   3.1s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   2.6s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   2.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   2.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   2.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   2.3s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   2.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   2.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   2.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   2.3s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   2.2s\n",
      "[CV] model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=None, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   2.3s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   1.5s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   1.7s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   1.7s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   1.9s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   1.7s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   1.6s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   1.5s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   1.4s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   1.5s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=2, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   1.4s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=mean, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=100, preprocessor__num__imputer__strategy=median, total=   0.1s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   1.5s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   1.4s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   1.5s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   1.4s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=mean, total=   1.5s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   1.6s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   1.5s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   1.5s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   1.6s\n",
      "[CV] model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median \n",
      "[CV]  model__max_depth=5, model__max_features=auto, model__min_samples_split=4, model__n_estimators=1000, preprocessor__num__imputer__strategy=median, total=   1.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('preprocessor', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('cat', Pipeline(memory=None,\n",
       "     steps=[('imputer', SimpleImputer(copy=True, fill_value='missing', missing_values=nan,\n",
       "       strategy='constant'...ators=10, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'preprocessor__num__imputer__strategy': ['mean', 'median'], 'model__n_estimators': [100, 1000], 'model__max_depth': [None, 5], 'model__max_features': ['auto'], 'model__min_samples_split': [2, 4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use GridSearchCV with our regression Pipeline\n",
    "\n",
    "pipe_grid={\n",
    "    \n",
    "    \"preprocessor__num__imputer__strategy\":[\"mean\",\"median\"],\n",
    "    \"model__n_estimators\":[100,1000],\n",
    "    \"model__max_depth\" :[None,5],\n",
    "    \"model__max_features\" :[\"auto\"],\n",
    "    \"model__min_samples_split\":[2,4]\n",
    "    \n",
    "}\n",
    "\n",
    "gs_model=GridSearchCV(model,pipe_grid,cv=5,verbose=2)\n",
    "gs_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3337859800130589"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0. An end to end scikit learn workflow',\n",
       " '1. Getting into data ready',\n",
       " '2. choose the right estimator/model/algo for problems',\n",
       " '3. Fit the model/algorithm and use it to make predictions on our data',\n",
       " '4. Evaluating a model',\n",
       " '5. Improve a model',\n",
       " '6. save and load a trained model',\n",
       " '7. Putting it all together']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "What_we_are_covering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Place : Scikitlearn : https://scikit-learn.org/stable/user_guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
